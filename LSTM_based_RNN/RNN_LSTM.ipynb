{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161772"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Memory usage (in bytes) before running program\n",
    "resource.getrusage(resource.RUSAGE_SELF).ru_maxrss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading data\n",
    "dataframe = pandas.read_csv('data_zigzag.csv', usecols=[1], engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123, 54)\n"
     ]
    }
   ],
   "source": [
    "#Train, Test set Split\n",
    "train_size = int(len(dataset) * 0.7)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to generate time series data matrix from given data\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reshape into X=t and Y=t+1 using create_dataset function defined above\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LSTM Sequential model takes input in form of [samples, time steps, features]\n",
    "#so reshape input to [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "0s - loss: 0.1024\n",
      "Epoch 2/200\n",
      "0s - loss: 0.0664\n",
      "Epoch 3/200\n",
      "0s - loss: 0.0590\n",
      "Epoch 4/200\n",
      "0s - loss: 0.0572\n",
      "Epoch 5/200\n",
      "0s - loss: 0.0568\n",
      "Epoch 6/200\n",
      "0s - loss: 0.0565\n",
      "Epoch 7/200\n",
      "0s - loss: 0.0562\n",
      "Epoch 8/200\n",
      "0s - loss: 0.0560\n",
      "Epoch 9/200\n",
      "0s - loss: 0.0554\n",
      "Epoch 10/200\n",
      "0s - loss: 0.0553\n",
      "Epoch 11/200\n",
      "0s - loss: 0.0549\n",
      "Epoch 12/200\n",
      "0s - loss: 0.0547\n",
      "Epoch 13/200\n",
      "0s - loss: 0.0542\n",
      "Epoch 14/200\n",
      "0s - loss: 0.0540\n",
      "Epoch 15/200\n",
      "0s - loss: 0.0538\n",
      "Epoch 16/200\n",
      "0s - loss: 0.0535\n",
      "Epoch 17/200\n",
      "0s - loss: 0.0530\n",
      "Epoch 18/200\n",
      "0s - loss: 0.0528\n",
      "Epoch 19/200\n",
      "0s - loss: 0.0525\n",
      "Epoch 20/200\n",
      "0s - loss: 0.0521\n",
      "Epoch 21/200\n",
      "0s - loss: 0.0519\n",
      "Epoch 22/200\n",
      "0s - loss: 0.0518\n",
      "Epoch 23/200\n",
      "0s - loss: 0.0514\n",
      "Epoch 24/200\n",
      "0s - loss: 0.0511\n",
      "Epoch 25/200\n",
      "0s - loss: 0.0508\n",
      "Epoch 26/200\n",
      "0s - loss: 0.0504\n",
      "Epoch 27/200\n",
      "0s - loss: 0.0503\n",
      "Epoch 28/200\n",
      "0s - loss: 0.0500\n",
      "Epoch 29/200\n",
      "0s - loss: 0.0497\n",
      "Epoch 30/200\n",
      "0s - loss: 0.0493\n",
      "Epoch 31/200\n",
      "0s - loss: 0.0491\n",
      "Epoch 32/200\n",
      "0s - loss: 0.0489\n",
      "Epoch 33/200\n",
      "0s - loss: 0.0487\n",
      "Epoch 34/200\n",
      "0s - loss: 0.0484\n",
      "Epoch 35/200\n",
      "0s - loss: 0.0481\n",
      "Epoch 36/200\n",
      "0s - loss: 0.0478\n",
      "Epoch 37/200\n",
      "0s - loss: 0.0475\n",
      "Epoch 38/200\n",
      "0s - loss: 0.0473\n",
      "Epoch 39/200\n",
      "0s - loss: 0.0471\n",
      "Epoch 40/200\n",
      "0s - loss: 0.0466\n",
      "Epoch 41/200\n",
      "0s - loss: 0.0463\n",
      "Epoch 42/200\n",
      "0s - loss: 0.0461\n",
      "Epoch 43/200\n",
      "0s - loss: 0.0459\n",
      "Epoch 44/200\n",
      "0s - loss: 0.0457\n",
      "Epoch 45/200\n",
      "0s - loss: 0.0453\n",
      "Epoch 46/200\n",
      "0s - loss: 0.0450\n",
      "Epoch 47/200\n",
      "0s - loss: 0.0447\n",
      "Epoch 48/200\n",
      "0s - loss: 0.0445\n",
      "Epoch 49/200\n",
      "0s - loss: 0.0442\n",
      "Epoch 50/200\n",
      "0s - loss: 0.0440\n",
      "Epoch 51/200\n",
      "0s - loss: 0.0438\n",
      "Epoch 52/200\n",
      "0s - loss: 0.0435\n",
      "Epoch 53/200\n",
      "0s - loss: 0.0432\n",
      "Epoch 54/200\n",
      "0s - loss: 0.0430\n",
      "Epoch 55/200\n",
      "0s - loss: 0.0428\n",
      "Epoch 56/200\n",
      "0s - loss: 0.0425\n",
      "Epoch 57/200\n",
      "0s - loss: 0.0421\n",
      "Epoch 58/200\n",
      "0s - loss: 0.0420\n",
      "Epoch 59/200\n",
      "0s - loss: 0.0419\n",
      "Epoch 60/200\n",
      "0s - loss: 0.0415\n",
      "Epoch 61/200\n",
      "0s - loss: 0.0412\n",
      "Epoch 62/200\n",
      "0s - loss: 0.0410\n",
      "Epoch 63/200\n",
      "0s - loss: 0.0409\n",
      "Epoch 64/200\n",
      "0s - loss: 0.0405\n",
      "Epoch 65/200\n",
      "0s - loss: 0.0403\n",
      "Epoch 66/200\n",
      "0s - loss: 0.0401\n",
      "Epoch 67/200\n",
      "0s - loss: 0.0399\n",
      "Epoch 68/200\n",
      "0s - loss: 0.0396\n",
      "Epoch 69/200\n",
      "0s - loss: 0.0395\n",
      "Epoch 70/200\n",
      "0s - loss: 0.0391\n",
      "Epoch 71/200\n",
      "0s - loss: 0.0391\n",
      "Epoch 72/200\n",
      "0s - loss: 0.0389\n",
      "Epoch 73/200\n",
      "0s - loss: 0.0385\n",
      "Epoch 74/200\n",
      "0s - loss: 0.0382\n",
      "Epoch 75/200\n",
      "0s - loss: 0.0382\n",
      "Epoch 76/200\n",
      "0s - loss: 0.0378\n",
      "Epoch 77/200\n",
      "0s - loss: 0.0377\n",
      "Epoch 78/200\n",
      "0s - loss: 0.0375\n",
      "Epoch 79/200\n",
      "0s - loss: 0.0372\n",
      "Epoch 80/200\n",
      "0s - loss: 0.0370\n",
      "Epoch 81/200\n",
      "0s - loss: 0.0369\n",
      "Epoch 82/200\n",
      "0s - loss: 0.0366\n",
      "Epoch 83/200\n",
      "0s - loss: 0.0364\n",
      "Epoch 84/200\n",
      "0s - loss: 0.0363\n",
      "Epoch 85/200\n",
      "0s - loss: 0.0361\n",
      "Epoch 86/200\n",
      "0s - loss: 0.0358\n",
      "Epoch 87/200\n",
      "0s - loss: 0.0357\n",
      "Epoch 88/200\n",
      "0s - loss: 0.0355\n",
      "Epoch 89/200\n",
      "0s - loss: 0.0352\n",
      "Epoch 90/200\n",
      "0s - loss: 0.0352\n",
      "Epoch 91/200\n",
      "0s - loss: 0.0349\n",
      "Epoch 92/200\n",
      "0s - loss: 0.0347\n",
      "Epoch 93/200\n",
      "0s - loss: 0.0346\n",
      "Epoch 94/200\n",
      "0s - loss: 0.0345\n",
      "Epoch 95/200\n",
      "0s - loss: 0.0343\n",
      "Epoch 96/200\n",
      "0s - loss: 0.0341\n",
      "Epoch 97/200\n",
      "0s - loss: 0.0339\n",
      "Epoch 98/200\n",
      "0s - loss: 0.0337\n",
      "Epoch 99/200\n",
      "0s - loss: 0.0336\n",
      "Epoch 100/200\n",
      "0s - loss: 0.0334\n",
      "Epoch 101/200\n",
      "0s - loss: 0.0333\n",
      "Epoch 102/200\n",
      "0s - loss: 0.0331\n",
      "Epoch 103/200\n",
      "0s - loss: 0.0330\n",
      "Epoch 104/200\n",
      "0s - loss: 0.0328\n",
      "Epoch 105/200\n",
      "0s - loss: 0.0327\n",
      "Epoch 106/200\n",
      "0s - loss: 0.0326\n",
      "Epoch 107/200\n",
      "0s - loss: 0.0324\n",
      "Epoch 108/200\n",
      "0s - loss: 0.0322\n",
      "Epoch 109/200\n",
      "0s - loss: 0.0321\n",
      "Epoch 110/200\n",
      "0s - loss: 0.0321\n",
      "Epoch 111/200\n",
      "0s - loss: 0.0319\n",
      "Epoch 112/200\n",
      "0s - loss: 0.0318\n",
      "Epoch 113/200\n",
      "0s - loss: 0.0318\n",
      "Epoch 114/200\n",
      "0s - loss: 0.0315\n",
      "Epoch 115/200\n",
      "0s - loss: 0.0314\n",
      "Epoch 116/200\n",
      "0s - loss: 0.0313\n",
      "Epoch 117/200\n",
      "0s - loss: 0.0311\n",
      "Epoch 118/200\n",
      "0s - loss: 0.0311\n",
      "Epoch 119/200\n",
      "0s - loss: 0.0309\n",
      "Epoch 120/200\n",
      "0s - loss: 0.0309\n",
      "Epoch 121/200\n",
      "0s - loss: 0.0308\n",
      "Epoch 122/200\n",
      "0s - loss: 0.0307\n",
      "Epoch 123/200\n",
      "0s - loss: 0.0306\n",
      "Epoch 124/200\n",
      "0s - loss: 0.0305\n",
      "Epoch 125/200\n",
      "0s - loss: 0.0304\n",
      "Epoch 126/200\n",
      "0s - loss: 0.0304\n",
      "Epoch 127/200\n",
      "0s - loss: 0.0303\n",
      "Epoch 128/200\n",
      "0s - loss: 0.0301\n",
      "Epoch 129/200\n",
      "0s - loss: 0.0301\n",
      "Epoch 130/200\n",
      "0s - loss: 0.0300\n",
      "Epoch 131/200\n",
      "0s - loss: 0.0300\n",
      "Epoch 132/200\n",
      "0s - loss: 0.0298\n",
      "Epoch 133/200\n",
      "0s - loss: 0.0298\n",
      "Epoch 134/200\n",
      "0s - loss: 0.0297\n",
      "Epoch 135/200\n",
      "0s - loss: 0.0296\n",
      "Epoch 136/200\n",
      "0s - loss: 0.0295\n",
      "Epoch 137/200\n",
      "0s - loss: 0.0295\n",
      "Epoch 138/200\n",
      "0s - loss: 0.0294\n",
      "Epoch 139/200\n",
      "0s - loss: 0.0293\n",
      "Epoch 140/200\n",
      "0s - loss: 0.0293\n",
      "Epoch 141/200\n",
      "0s - loss: 0.0291\n",
      "Epoch 142/200\n",
      "0s - loss: 0.0291\n",
      "Epoch 143/200\n",
      "0s - loss: 0.0290\n",
      "Epoch 144/200\n",
      "0s - loss: 0.0290\n",
      "Epoch 145/200\n",
      "0s - loss: 0.0289\n",
      "Epoch 146/200\n",
      "0s - loss: 0.0289\n",
      "Epoch 147/200\n",
      "0s - loss: 0.0289\n",
      "Epoch 148/200\n",
      "0s - loss: 0.0288\n",
      "Epoch 149/200\n",
      "0s - loss: 0.0287\n",
      "Epoch 150/200\n",
      "0s - loss: 0.0288\n",
      "Epoch 151/200\n",
      "0s - loss: 0.0287\n",
      "Epoch 152/200\n",
      "0s - loss: 0.0286\n",
      "Epoch 153/200\n",
      "0s - loss: 0.0286\n",
      "Epoch 154/200\n",
      "0s - loss: 0.0285\n",
      "Epoch 155/200\n",
      "0s - loss: 0.0284\n",
      "Epoch 156/200\n",
      "0s - loss: 0.0284\n",
      "Epoch 157/200\n",
      "0s - loss: 0.0283\n",
      "Epoch 158/200\n",
      "0s - loss: 0.0283\n",
      "Epoch 159/200\n",
      "0s - loss: 0.0283\n",
      "Epoch 160/200\n",
      "0s - loss: 0.0283\n",
      "Epoch 161/200\n",
      "0s - loss: 0.0283\n",
      "Epoch 162/200\n",
      "0s - loss: 0.0282\n",
      "Epoch 163/200\n",
      "0s - loss: 0.0282\n",
      "Epoch 164/200\n",
      "0s - loss: 0.0281\n",
      "Epoch 165/200\n",
      "0s - loss: 0.0281\n",
      "Epoch 166/200\n",
      "0s - loss: 0.0280\n",
      "Epoch 167/200\n",
      "0s - loss: 0.0281\n",
      "Epoch 168/200\n",
      "0s - loss: 0.0280\n",
      "Epoch 169/200\n",
      "0s - loss: 0.0279\n",
      "Epoch 170/200\n",
      "0s - loss: 0.0279\n",
      "Epoch 171/200\n",
      "0s - loss: 0.0279\n",
      "Epoch 172/200\n",
      "0s - loss: 0.0279\n",
      "Epoch 173/200\n",
      "0s - loss: 0.0279\n",
      "Epoch 174/200\n",
      "0s - loss: 0.0279\n",
      "Epoch 175/200\n",
      "0s - loss: 0.0279\n",
      "Epoch 176/200\n",
      "0s - loss: 0.0277\n",
      "Epoch 177/200\n",
      "0s - loss: 0.0277\n",
      "Epoch 178/200\n",
      "0s - loss: 0.0278\n",
      "Epoch 179/200\n",
      "0s - loss: 0.0277\n",
      "Epoch 180/200\n",
      "0s - loss: 0.0277\n",
      "Epoch 181/200\n",
      "0s - loss: 0.0277\n",
      "Epoch 182/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 183/200\n",
      "0s - loss: 0.0276\n",
      "Epoch 184/200\n",
      "0s - loss: 0.0277\n",
      "Epoch 185/200\n",
      "0s - loss: 0.0276\n",
      "Epoch 186/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 187/200\n",
      "0s - loss: 0.0276\n",
      "Epoch 188/200\n",
      "0s - loss: 0.0276\n",
      "Epoch 189/200\n",
      "0s - loss: 0.0276\n",
      "Epoch 190/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 191/200\n",
      "0s - loss: 0.0276\n",
      "Epoch 192/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 193/200\n",
      "0s - loss: 0.0276\n",
      "Epoch 194/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 195/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 196/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 197/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 198/200\n",
      "0s - loss: 0.0274\n",
      "Epoch 199/200\n",
      "0s - loss: 0.0275\n",
      "Epoch 200/200\n",
      "0s - loss: 0.0275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6390fcc4d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Sequential Model from Keras library used\n",
    "model = Sequential()\n",
    "\n",
    "#add LSTM layer and a regular densely-connected NN layer\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1)) #fully connected\n",
    "\n",
    "#Compile, set optimizer to adam and then fit the training data\n",
    "#optimizer used = adam, SGD, RMSprop, Adagrad, Adadelta, Adamax, Nadam\n",
    "model.compile(loss='mean_squared_error', optimizer='SGD') \n",
    "\n",
    "time_start = time.clock()\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36248112]\n",
      " [ 0.41449237]\n",
      " [ 0.46491492]\n",
      " [ 0.51332277]\n",
      " [ 0.55937618]\n",
      " [ 0.51332277]\n",
      " [ 0.46491492]\n",
      " [ 0.41449237]\n",
      " [ 0.36248112]\n",
      " [ 0.30937535]\n",
      " [ 0.25571305]\n",
      " [ 0.20204553]\n",
      " [ 0.14890726]\n",
      " [ 0.09678832]\n",
      " [ 0.55937618]\n",
      " [ 0.51332277]\n",
      " [ 0.46491492]\n",
      " [ 0.41449237]\n",
      " [ 0.36248112]\n",
      " [ 0.30937535]\n",
      " [ 0.25571305]\n",
      " [ 0.20204553]\n",
      " [ 0.14890726]\n",
      " [ 0.09678832]\n",
      " [ 0.55937618]\n",
      " [ 0.51332277]\n",
      " [ 0.46491492]\n",
      " [ 0.41449237]\n",
      " [ 0.36248112]\n",
      " [ 0.30937535]\n",
      " [ 0.25571305]\n",
      " [ 0.20204553]\n",
      " [ 0.14890726]\n",
      " [ 0.09678832]\n",
      " [ 0.30937535]\n",
      " [ 0.25571305]\n",
      " [ 0.20204553]\n",
      " [ 0.14890726]\n",
      " [ 0.09678832]\n",
      " [ 0.14890726]\n",
      " [ 0.20204553]\n",
      " [ 0.25571305]\n",
      " [ 0.30937535]\n",
      " [ 0.09678832]\n",
      " [ 0.14890726]\n",
      " [ 0.20204553]\n",
      " [ 0.25571305]\n",
      " [ 0.30937535]\n",
      " [ 0.09678832]\n",
      " [ 0.14890726]\n",
      " [ 0.20204553]\n",
      " [ 0.25571305]]\n"
     ]
    }
   ],
   "source": [
    "#Predicting for test set & train set and calculating loss \n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "#inverting predictions before calculating error so that performance is reported in the same units as the original data\n",
    "#reverse of normalizing\n",
    "print testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 2.31 RMSE\n",
      "Test Score: 2.08 RMSE\n"
     ]
    }
   ],
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "#calculating root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.07473516]\n",
      " [ 6.80289268]\n",
      " [ 7.50880909]\n",
      " [ 8.18651867]\n",
      " [ 8.8312664 ]\n",
      " [ 8.18651867]\n",
      " [ 7.50880909]\n",
      " [ 6.80289268]\n",
      " [ 6.07473516]\n",
      " [ 5.33125448]\n",
      " [ 4.57998228]\n",
      " [ 3.82863712]\n",
      " [ 3.08470154]\n",
      " [ 2.35503626]\n",
      " [ 8.8312664 ]\n",
      " [ 8.18651867]\n",
      " [ 7.50880909]\n",
      " [ 6.80289268]\n",
      " [ 6.07473516]\n",
      " [ 5.33125448]\n",
      " [ 4.57998228]\n",
      " [ 3.82863712]\n",
      " [ 3.08470154]\n",
      " [ 2.35503626]\n",
      " [ 8.8312664 ]\n",
      " [ 8.18651867]\n",
      " [ 7.50880909]\n",
      " [ 6.80289268]\n",
      " [ 6.07473516]\n",
      " [ 5.33125448]\n",
      " [ 4.57998228]\n",
      " [ 3.82863712]\n",
      " [ 3.08470154]\n",
      " [ 2.35503626]\n",
      " [ 5.33125448]\n",
      " [ 4.57998228]\n",
      " [ 3.82863712]\n",
      " [ 3.08470154]\n",
      " [ 2.35503626]\n",
      " [ 3.08470154]\n",
      " [ 3.82863712]\n",
      " [ 4.57998228]\n",
      " [ 5.33125448]\n",
      " [ 2.35503626]\n",
      " [ 3.08470154]\n",
      " [ 3.82863712]\n",
      " [ 4.57998228]\n",
      " [ 5.33125448]\n",
      " [ 2.35503626]\n",
      " [ 3.08470154]\n",
      " [ 3.82863712]\n",
      " [ 4.57998228]]\n"
     ]
    }
   ],
   "source": [
    "print testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time taken by the whole process\n",
    "time_elapsed = (time.clock() - time_start)\n",
    "time_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Shifting axis for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[0:len(trainPredict), :] = trainPredict\n",
    "#trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back)+1:len(dataset)-1-(look_back), :] = testPredict\n",
    "#testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "#Plotting baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory usage (in bytes) after running program\n",
    "resource.getrusage(resource.RUSAGE_SELF).ru_maxrss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(trainPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(trainPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
